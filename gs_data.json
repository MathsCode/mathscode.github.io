{"container_type": "Author", "filled": ["basics", "publications", "indices", "counts"], "scholar_id": "ovQFaicAAAAJ", "source": "AUTHOR_PROFILE_PAGE", "name": "Jiaming Xu", "url_picture": "https://scholar.googleusercontent.com/citations?view_op=view_photo&user=ovQFaicAAAAJ&citpid=1", "affiliation": "Shanghai Jiao Tong University", "organization": 11817969702535318711, "interests": ["Efficient AI", "System and Architecture", "GPU", "LLM"], "email_domain": "@sjtu.edu.cn", "homepage": "http://mathscode.github.io/", "citedby": 655, "publications": {"ovQFaicAAAAJ:2osOgNQ5qMEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "A survey on efficient inference for large language models", "pub_year": "2024"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:2osOgNQ5qMEC", "num_citations": 324, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14173509981179667062,17250282568479785624,1149916255048800095", "cites_id": ["14173509981179667062", "17250282568479785624", "1149916255048800095"]}, "ovQFaicAAAAJ:qjMakFHDy7sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Flashdecoding++: Faster large language model inference with asynchronization, flat gemm optimization, and heuristics", "pub_year": "2024"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:qjMakFHDy7sC", "num_citations": 157, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4314979185188623256,1999727562417019736,18294979008990233962,17836632044074841285,827676612157231938", "cites_id": ["4314979185188623256", "1999727562417019736", "18294979008990233962", "17836632044074841285", "827676612157231938"]}, "ovQFaicAAAAJ:UeHWp8X0CEIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Large language model inference acceleration: A comprehensive hardware perspective", "pub_year": "2024"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:UeHWp8X0CEIC", "num_citations": 85, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13488907632503633475", "cites_id": ["13488907632503633475"]}, "ovQFaicAAAAJ:IjCSPb-OGe4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Marca: Mamba accelerator with reconfigurable architecture", "pub_year": "2024"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:IjCSPb-OGe4C", "num_citations": 23, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8363143917185082995", "cites_id": ["8363143917185082995"]}, "ovQFaicAAAAJ:WF5omc3nYNoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Specee: Accelerating large language model inference with speculative early exiting", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:WF5omc3nYNoC", "num_citations": 16, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1905355922858332632", "cites_id": ["1905355922858332632"]}, "ovQFaicAAAAJ:eQOLeE2rZwMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Fast and Efficient 2-bit LLM Inference on GPU: 2/4/16-bit in a Weight Matrix with Asynchronous Dequantization", "pub_year": "2024"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:eQOLeE2rZwMC", "num_citations": 16, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17065362057960297953,12340343822247497170", "cites_id": ["17065362057960297953", "12340343822247497170"]}, "ovQFaicAAAAJ:0EnyYjriUFMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Specprune-vla: Accelerating vision-language-action models via action-aware self-speculative pruning", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:0EnyYjriUFMC", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6494835281003186600", "cites_id": ["6494835281003186600"]}, "ovQFaicAAAAJ:d1gkVwhDpl0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Tstc: Two-level sparsity tensor core enabling both algorithm flexibility and hardware efficiency", "pub_year": "2023"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:d1gkVwhDpl0C", "num_citations": 8, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=3199193296279589730", "cites_id": ["3199193296279589730"]}, "ovQFaicAAAAJ:Tyk-4Ss8FVUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Enabling efficient sparse multiplications on gpus with heuristic adaptability", "pub_year": "2024"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:Tyk-4Ss8FVUC", "num_citations": 4, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5103373072664657745", "cites_id": ["5103373072664657745"]}, "ovQFaicAAAAJ:W7OEmFMy1HYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "LLSM: LLM-enhanced logic synthesis model with EDA-guided CoT prompting, hybrid embedding and AIG-tailored acceleration", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:W7OEmFMy1HYC", "num_citations": 3, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2010028133144721317", "cites_id": ["2010028133144721317"]}, "ovQFaicAAAAJ:MXK_kJrjxJIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "A Cross-model Fusion-aware Framework for Optimizing (gather-matmul-scatter)s Workload", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:MXK_kJrjxJIC", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1854124633879766307", "cites_id": ["1854124633879766307"]}, "ovQFaicAAAAJ:KlAtU1dfN6UC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MARCA-v2: Mamba Accelerator with Complementary State Space Model Sparsity and Reconfigurable Architecture", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:KlAtU1dfN6UC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11775844693721937910", "cites_id": ["11775844693721937910"]}, "ovQFaicAAAAJ:5nxA0vEk-isC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:5nxA0vEk-isC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14323238939248813000", "cites_id": ["14323238939248813000"]}, "ovQFaicAAAAJ:Se3iqnhoufwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "FlashDecoding++Next: High Throughput LLM Inference with Latency and Memory Optimization", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:Se3iqnhoufwC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16438088294924404222", "cites_id": ["16438088294924404222"]}, "ovQFaicAAAAJ:ULOm3_A8WrAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:ULOm3_A8WrAC", "num_citations": 0}, "ovQFaicAAAAJ:Zph67rFs4hoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SG-Filter: Enhancing Similar Text Retrieval via Hierarchical Summarized-Semantic Index and Adaptive Filtering", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:Zph67rFs4hoC", "num_citations": 0}, "ovQFaicAAAAJ:kNdYIx-mwKoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:kNdYIx-mwKoC", "num_citations": 0}, "ovQFaicAAAAJ:UebtZRa9Y70C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "DyLGNN: Efficient LM-GNN Fine-Tuning with Dynamic Node Partitioning, Low-Degree Sparsity, and Asynchronous Sub-Batch", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:UebtZRa9Y70C", "num_citations": 0}, "ovQFaicAAAAJ:YsMSGLbcyi4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Accelerator for LLM-Enhanced GNN with Product Quantization and Unified Indexing", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:YsMSGLbcyi4C", "num_citations": 0}, "ovQFaicAAAAJ:YOwf2qJgpHMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SpAct-NDP: Efficient LLM Inference via Sparse Activation on NDP-GPU Heterogeneous Architecture"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:YOwf2qJgpHMC", "num_citations": 0}}, "citedby5y": 655, "hindex": 8, "hindex5y": 8, "i10index": 7, "i10index5y": 7, "cites_per_year": {"2023": 3, "2024": 127, "2025": 469, "2026": 56}, "updated": "2026-02-19 08:38:36.005475"}