{"container_type": "Author", "filled": ["basics", "publications", "indices", "counts"], "scholar_id": "ovQFaicAAAAJ", "source": "AUTHOR_PROFILE_PAGE", "name": "Jiaming Xu", "url_picture": "https://scholar.googleusercontent.com/citations?view_op=view_photo&user=ovQFaicAAAAJ&citpid=1", "affiliation": "Shanghai Jiao Tong University", "organization": 11817969702535318711, "interests": ["System and Architecture", "GPU", "Deep Learning", "LLM"], "email_domain": "@sjtu.edu.cn", "homepage": "http://mathscode.github.io/", "citedby": 472, "publications": {"ovQFaicAAAAJ:2osOgNQ5qMEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "A survey on efficient inference for large language models", "pub_year": "2024"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:2osOgNQ5qMEC", "num_citations": 243, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14173509981179667062,17250282568479785624,1149916255048800095", "cites_id": ["14173509981179667062", "17250282568479785624", "1149916255048800095"]}, "ovQFaicAAAAJ:qjMakFHDy7sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Flashdecoding++: Faster large language model inference with asynchronization, flat gemm optimization, and heuristics", "pub_year": "2024"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:qjMakFHDy7sC", "num_citations": 125, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4314979185188623256,1999727562417019736,18294979008990233962,17836632044074841285,827676612157231938", "cites_id": ["4314979185188623256", "1999727562417019736", "18294979008990233962", "17836632044074841285", "827676612157231938"]}, "ovQFaicAAAAJ:UeHWp8X0CEIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Large language model inference acceleration: A comprehensive hardware perspective", "pub_year": "2024"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:UeHWp8X0CEIC", "num_citations": 54, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13488907632503633475", "cites_id": ["13488907632503633475"]}, "ovQFaicAAAAJ:IjCSPb-OGe4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Marca: Mamba accelerator with reconfigurable architecture", "pub_year": "2024"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:IjCSPb-OGe4C", "num_citations": 16, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8363143917185082995", "cites_id": ["8363143917185082995"]}, "ovQFaicAAAAJ:eQOLeE2rZwMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Fast and Efficient 2-bit LLM Inference on GPU: 2/4/16-bit in a Weight Matrix with Asynchronous Dequantization", "pub_year": "2024"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:eQOLeE2rZwMC", "num_citations": 13, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17065362057960297953,12340343822247497170", "cites_id": ["17065362057960297953", "12340343822247497170"]}, "ovQFaicAAAAJ:WF5omc3nYNoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Specee: Accelerating large language model inference with speculative early exiting", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:WF5omc3nYNoC", "num_citations": 7, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1905355922858332632", "cites_id": ["1905355922858332632"]}, "ovQFaicAAAAJ:d1gkVwhDpl0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Tstc: Two-level sparsity tensor core enabling both algorithm flexibility and hardware efficiency", "pub_year": "2023"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:d1gkVwhDpl0C", "num_citations": 7, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=3199193296279589730", "cites_id": ["3199193296279589730"]}, "ovQFaicAAAAJ:MXK_kJrjxJIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "A Cross-model Fusion-aware Framework for Optimizing (gather-matmul-scatter)s Workload", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:MXK_kJrjxJIC", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1854124633879766307", "cites_id": ["1854124633879766307"]}, "ovQFaicAAAAJ:W7OEmFMy1HYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Llsm: Llm-enhanced logic synthesis model with eda-guided cot prompting, hybrid embedding and aig-tailored acceleration", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:W7OEmFMy1HYC", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2010028133144721317", "cites_id": ["2010028133144721317"]}, "ovQFaicAAAAJ:Tyk-4Ss8FVUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Enabling efficient sparse multiplications on GPUs with heuristic adaptability", "pub_year": "2024"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:Tyk-4Ss8FVUC", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5103373072664657745", "cites_id": ["5103373072664657745"]}, "ovQFaicAAAAJ:0EnyYjriUFMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Specprune-vla: Accelerating vision-language-action models via action-aware self-speculative pruning", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:0EnyYjriUFMC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6494835281003186600", "cites_id": ["6494835281003186600"]}, "ovQFaicAAAAJ:5nxA0vEk-isC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:5nxA0vEk-isC", "num_citations": 0}, "ovQFaicAAAAJ:Se3iqnhoufwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "FlashDecoding++Next: High Throughput LLM Inference with Latency and Memory Optimization", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:Se3iqnhoufwC", "num_citations": 0}, "ovQFaicAAAAJ:UebtZRa9Y70C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "DyLGNN: Efficient LM-GNN Fine-Tuning with Dynamic Node Partitioning, Low-Degree Sparsity, and Asynchronous Sub-Batch", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:UebtZRa9Y70C", "num_citations": 0}, "ovQFaicAAAAJ:YsMSGLbcyi4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Accelerator for LLM-Enhanced GNN with Product Quantization and Unified Indexing", "pub_year": "2025"}, "filled": false, "author_pub_id": "ovQFaicAAAAJ:YsMSGLbcyi4C", "num_citations": 0}}, "citedby5y": 472, "hindex": 7, "hindex5y": 7, "i10index": 5, "i10index5y": 5, "cites_per_year": {"2023": 3, "2024": 134, "2025": 335}, "updated": "2025-10-26 08:18:53.478944"}